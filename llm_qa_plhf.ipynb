{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dave-does-data/llm-qa-rlhf/blob/main/llm_qa_plhf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "62mgWR1fQ94z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/dave-does-data/llm-qa-rlhf.git\n",
        "!pip install -r llm-qa-rlhf/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgo594cZnoTr",
        "outputId": "019f0eba-85da-4066-a1a3-61877cf9c3be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "I-7TCRQpVaJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3401409-3ab7-486d-e55f-7865b9de68d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdave-does-data\u001b[0m (\u001b[33mutsa-it-phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python llm-qa-rlhf/src/trl/sft_trainer.py \\\n",
        "    --model_name meta-llama/Llama-2-7b-hf \\\n",
        "    --dataset_name dave-does-data/databricks-dolly-qa-subset-7k \\\n",
        "    --log_with 'wandb' \\\n",
        "    --load_in_8bit \\\n",
        "    --use_peft \\\n",
        "    --batch_size 1 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --max_steps 10"
      ],
      "metadata": {
        "id": "M86KHvVXgxzh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc83ba20-1ae4-461a-8097-b4a6291e1223"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-25 20:23:38.767187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 2/2 [01:09<00:00, 34.75s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "Using pad_token, but it is not set yet.\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdave-does-data\u001b[0m (\u001b[33mutsa-it-phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230725_202504-utkd6g7v\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclean-paper-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utsa-it-phd/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utsa-it-phd/huggingface/runs/utkd6g7v\u001b[0m\n",
            "  0% 0/2889 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 1.9024, 'learning_rate': 1.4095119418483906e-05, 'epoch': 0.0}\n",
            "{'loss': 2.4791, 'learning_rate': 1.4090238836967809e-05, 'epoch': 0.0}\n",
            "{'loss': 2.2341, 'learning_rate': 1.4085358255451714e-05, 'epoch': 0.0}\n",
            "{'loss': 2.4727, 'learning_rate': 1.4080477673935617e-05, 'epoch': 0.0}\n",
            "{'loss': 2.4329, 'learning_rate': 1.4075597092419524e-05, 'epoch': 0.01}\n",
            "{'loss': 2.2214, 'learning_rate': 1.4070716510903427e-05, 'epoch': 0.01}\n",
            "{'loss': 2.4591, 'learning_rate': 1.4065835929387332e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1184, 'learning_rate': 1.4060955347871236e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1322, 'learning_rate': 1.405607476635514e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1108, 'learning_rate': 1.4051194184839046e-05, 'epoch': 0.01}\n",
            "{'loss': 2.2306, 'learning_rate': 1.4046313603322949e-05, 'epoch': 0.01}\n",
            "{'loss': 2.2232, 'learning_rate': 1.4041433021806854e-05, 'epoch': 0.01}\n",
            "{'loss': 2.0809, 'learning_rate': 1.4036552440290757e-05, 'epoch': 0.01}\n",
            "{'loss': 2.0422, 'learning_rate': 1.4031671858774664e-05, 'epoch': 0.01}\n",
            "{'loss': 2.3918, 'learning_rate': 1.4026791277258568e-05, 'epoch': 0.02}\n",
            "{'loss': 2.1498, 'learning_rate': 1.4021910695742473e-05, 'epoch': 0.02}\n",
            "{'loss': 2.2579, 'learning_rate': 1.4017030114226376e-05, 'epoch': 0.02}\n",
            "{'loss': 1.763, 'learning_rate': 1.4012149532710281e-05, 'epoch': 0.02}\n",
            "{'loss': 1.8333, 'learning_rate': 1.4007268951194186e-05, 'epoch': 0.02}\n",
            "{'loss': 2.0147, 'learning_rate': 1.400238836967809e-05, 'epoch': 0.02}\n",
            "{'loss': 2.0803, 'learning_rate': 1.3997507788161994e-05, 'epoch': 0.02}\n",
            "{'loss': 2.1629, 'learning_rate': 1.3992627206645898e-05, 'epoch': 0.02}\n",
            "{'loss': 2.3389, 'learning_rate': 1.3987746625129804e-05, 'epoch': 0.02}\n",
            "{'loss': 2.1513, 'learning_rate': 1.3982866043613708e-05, 'epoch': 0.02}\n",
            "{'loss': 2.1865, 'learning_rate': 1.3977985462097613e-05, 'epoch': 0.03}\n",
            "{'loss': 2.5108, 'learning_rate': 1.3973104880581516e-05, 'epoch': 0.03}\n",
            "{'loss': 2.0238, 'learning_rate': 1.3968224299065421e-05, 'epoch': 0.03}\n",
            "{'loss': 2.1013, 'learning_rate': 1.3963343717549326e-05, 'epoch': 0.03}\n",
            "{'loss': 1.9095, 'learning_rate': 1.395846313603323e-05, 'epoch': 0.03}\n",
            "{'loss': 2.1362, 'learning_rate': 1.3953582554517135e-05, 'epoch': 0.03}\n",
            "{'loss': 1.9378, 'learning_rate': 1.3948701973001038e-05, 'epoch': 0.03}\n",
            "{'loss': 2.0533, 'learning_rate': 1.3943821391484945e-05, 'epoch': 0.03}\n",
            "{'loss': 1.8428, 'learning_rate': 1.3938940809968848e-05, 'epoch': 0.03}\n",
            "{'loss': 1.9008, 'learning_rate': 1.3934060228452753e-05, 'epoch': 0.04}\n",
            "{'loss': 2.0574, 'learning_rate': 1.3929179646936656e-05, 'epoch': 0.04}\n",
            "{'loss': 2.077, 'learning_rate': 1.3924299065420561e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9389, 'learning_rate': 1.3919418483904466e-05, 'epoch': 0.04}\n",
            "{'loss': 2.0566, 'learning_rate': 1.391453790238837e-05, 'epoch': 0.04}\n",
            "{'loss': 2.4625, 'learning_rate': 1.3909657320872275e-05, 'epoch': 0.04}\n",
            "{'loss': 2.1723, 'learning_rate': 1.3904776739356178e-05, 'epoch': 0.04}\n",
            "{'loss': 2.0071, 'learning_rate': 1.3899896157840083e-05, 'epoch': 0.04}\n",
            "{'loss': 2.048, 'learning_rate': 1.3895015576323988e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9775, 'learning_rate': 1.3890134994807893e-05, 'epoch': 0.04}\n",
            "{'loss': 1.8612, 'learning_rate': 1.3885254413291796e-05, 'epoch': 0.05}\n",
            "{'loss': 2.1039, 'learning_rate': 1.3880373831775702e-05, 'epoch': 0.05}\n",
            "{'loss': 1.9984, 'learning_rate': 1.3875493250259607e-05, 'epoch': 0.05}\n",
            "{'loss': 1.8495, 'learning_rate': 1.387061266874351e-05, 'epoch': 0.05}\n",
            "{'loss': 1.5912, 'learning_rate': 1.3865732087227415e-05, 'epoch': 0.05}\n",
            "{'loss': 2.1736, 'learning_rate': 1.3860851505711318e-05, 'epoch': 0.05}\n",
            "{'loss': 1.9864, 'learning_rate': 1.3855970924195223e-05, 'epoch': 0.05}\n",
            "{'loss': 1.5534, 'learning_rate': 1.3851090342679128e-05, 'epoch': 0.05}\n",
            "{'loss': 2.2928, 'learning_rate': 1.3846209761163033e-05, 'epoch': 0.05}\n",
            "{'loss': 1.929, 'learning_rate': 1.3841329179646937e-05, 'epoch': 0.06}\n",
            "{'loss': 1.7764, 'learning_rate': 1.3836448598130842e-05, 'epoch': 0.06}\n",
            "{'loss': 2.0259, 'learning_rate': 1.3831568016614747e-05, 'epoch': 0.06}\n",
            "{'loss': 2.1309, 'learning_rate': 1.382668743509865e-05, 'epoch': 0.06}\n",
            "{'loss': 1.9192, 'learning_rate': 1.3821806853582555e-05, 'epoch': 0.06}\n",
            "{'loss': 1.8727, 'learning_rate': 1.3816926272066458e-05, 'epoch': 0.06}\n",
            "{'loss': 1.7621, 'learning_rate': 1.3812045690550363e-05, 'epoch': 0.06}\n",
            "{'loss': 1.9552, 'learning_rate': 1.3807165109034268e-05, 'epoch': 0.06}\n",
            "{'loss': 2.0291, 'learning_rate': 1.3802284527518174e-05, 'epoch': 0.06}\n",
            "{'loss': 2.0296, 'learning_rate': 1.3797403946002077e-05, 'epoch': 0.06}\n",
            "{'loss': 2.1757, 'learning_rate': 1.3792523364485982e-05, 'epoch': 0.07}\n",
            "{'loss': 1.9806, 'learning_rate': 1.3787642782969887e-05, 'epoch': 0.07}\n",
            "{'loss': 1.9636, 'learning_rate': 1.378276220145379e-05, 'epoch': 0.07}\n",
            "{'loss': 2.2837, 'learning_rate': 1.3777881619937695e-05, 'epoch': 0.07}\n",
            "{'loss': 1.97, 'learning_rate': 1.3773001038421599e-05, 'epoch': 0.07}\n",
            "{'loss': 1.6955, 'learning_rate': 1.3768120456905504e-05, 'epoch': 0.07}\n",
            "{'loss': 2.0175, 'learning_rate': 1.3763239875389409e-05, 'epoch': 0.07}\n",
            "{'loss': 2.0034, 'learning_rate': 1.3758359293873314e-05, 'epoch': 0.07}\n",
            "{'loss': 1.7001, 'learning_rate': 1.3753478712357217e-05, 'epoch': 0.07}\n",
            "{'loss': 1.9022, 'learning_rate': 1.3748598130841122e-05, 'epoch': 0.07}\n",
            "{'loss': 1.6971, 'learning_rate': 1.3743717549325027e-05, 'epoch': 0.08}\n",
            "{'loss': 2.0715, 'learning_rate': 1.373883696780893e-05, 'epoch': 0.08}\n",
            "{'loss': 2.2948, 'learning_rate': 1.3733956386292835e-05, 'epoch': 0.08}\n",
            "{'loss': 1.6048, 'learning_rate': 1.3729075804776739e-05, 'epoch': 0.08}\n",
            "{'loss': 1.707, 'learning_rate': 1.3724195223260644e-05, 'epoch': 0.08}\n",
            "{'loss': 1.8417, 'learning_rate': 1.3719314641744549e-05, 'epoch': 0.08}\n",
            "{'loss': 1.8413, 'learning_rate': 1.3714434060228454e-05, 'epoch': 0.08}\n",
            "{'loss': 2.2435, 'learning_rate': 1.3709553478712357e-05, 'epoch': 0.08}\n",
            "{'loss': 1.9101, 'learning_rate': 1.3704672897196262e-05, 'epoch': 0.08}\n",
            "{'loss': 2.0587, 'learning_rate': 1.3699792315680167e-05, 'epoch': 0.09}\n",
            "{'loss': 2.1139, 'learning_rate': 1.369491173416407e-05, 'epoch': 0.09}\n",
            "{'loss': 1.9502, 'learning_rate': 1.3690031152647976e-05, 'epoch': 0.09}\n",
            "{'loss': 1.9851, 'learning_rate': 1.3685150571131879e-05, 'epoch': 0.09}\n",
            "{'loss': 1.8327, 'learning_rate': 1.3680269989615784e-05, 'epoch': 0.09}\n",
            "{'loss': 2.2128, 'learning_rate': 1.3675389408099689e-05, 'epoch': 0.09}\n",
            "{'loss': 1.4895, 'learning_rate': 1.3670508826583594e-05, 'epoch': 0.09}\n",
            "{'loss': 2.0604, 'learning_rate': 1.3665628245067497e-05, 'epoch': 0.09}\n",
            "{'loss': 1.3562, 'learning_rate': 1.3660747663551402e-05, 'epoch': 0.09}\n",
            "{'loss': 1.9461, 'learning_rate': 1.3655867082035308e-05, 'epoch': 0.09}\n",
            "{'loss': 2.1568, 'learning_rate': 1.3650986500519211e-05, 'epoch': 0.1}\n",
            "{'loss': 1.9053, 'learning_rate': 1.3646105919003116e-05, 'epoch': 0.1}\n",
            "{'loss': 1.7612, 'learning_rate': 1.364122533748702e-05, 'epoch': 0.1}\n",
            "{'loss': 2.083, 'learning_rate': 1.3636344755970924e-05, 'epoch': 0.1}\n",
            "{'loss': 1.6532, 'learning_rate': 1.363146417445483e-05, 'epoch': 0.1}\n",
            "{'loss': 2.0009, 'learning_rate': 1.3626583592938734e-05, 'epoch': 0.1}\n",
            "{'loss': 1.7351, 'learning_rate': 1.3621703011422638e-05, 'epoch': 0.1}\n",
            "{'loss': 1.8201, 'learning_rate': 1.3616822429906543e-05, 'epoch': 0.1}\n",
            "{'loss': 1.8934, 'learning_rate': 1.3611941848390448e-05, 'epoch': 0.1}\n",
            "{'loss': 1.8233, 'learning_rate': 1.3607061266874351e-05, 'epoch': 0.1}\n",
            "{'loss': 1.2201, 'learning_rate': 1.3602180685358256e-05, 'epoch': 0.11}\n",
            "{'loss': 1.7825, 'learning_rate': 1.359730010384216e-05, 'epoch': 0.11}\n",
            "{'loss': 1.9262, 'learning_rate': 1.3592419522326064e-05, 'epoch': 0.11}\n",
            "{'loss': 1.5309, 'learning_rate': 1.358753894080997e-05, 'epoch': 0.11}\n",
            "{'loss': 1.7477, 'learning_rate': 1.3582658359293875e-05, 'epoch': 0.11}\n",
            "{'loss': 1.7847, 'learning_rate': 1.3577777777777778e-05, 'epoch': 0.11}\n",
            "{'loss': 1.431, 'learning_rate': 1.3572897196261683e-05, 'epoch': 0.11}\n",
            "{'loss': 1.9034, 'learning_rate': 1.3568016614745588e-05, 'epoch': 0.11}\n",
            "{'loss': 1.2351, 'learning_rate': 1.3563136033229491e-05, 'epoch': 0.11}\n",
            "{'loss': 1.7673, 'learning_rate': 1.3558255451713396e-05, 'epoch': 0.12}\n",
            "{'loss': 1.7922, 'learning_rate': 1.35533748701973e-05, 'epoch': 0.12}\n",
            "{'loss': 1.514, 'learning_rate': 1.3548494288681205e-05, 'epoch': 0.12}\n",
            "{'loss': 1.5746, 'learning_rate': 1.354361370716511e-05, 'epoch': 0.12}\n",
            "{'loss': 1.9199, 'learning_rate': 1.3538733125649015e-05, 'epoch': 0.12}\n",
            "{'loss': 1.7746, 'learning_rate': 1.3533852544132918e-05, 'epoch': 0.12}\n",
            "{'loss': 1.9854, 'learning_rate': 1.3528971962616823e-05, 'epoch': 0.12}\n",
            "{'loss': 1.464, 'learning_rate': 1.3524091381100728e-05, 'epoch': 0.12}\n",
            "{'loss': 1.8186, 'learning_rate': 1.3519210799584631e-05, 'epoch': 0.12}\n",
            "{'loss': 1.4131, 'learning_rate': 1.3514330218068536e-05, 'epoch': 0.12}\n",
            "{'loss': 1.6463, 'learning_rate': 1.350944963655244e-05, 'epoch': 0.13}\n",
            "{'loss': 1.7428, 'learning_rate': 1.3504569055036345e-05, 'epoch': 0.13}\n",
            "{'loss': 1.3815, 'learning_rate': 1.349968847352025e-05, 'epoch': 0.13}\n",
            "{'loss': 1.7518, 'learning_rate': 1.3494807892004155e-05, 'epoch': 0.13}\n",
            "{'loss': 1.6572, 'learning_rate': 1.3489927310488058e-05, 'epoch': 0.13}\n",
            "{'loss': 1.8153, 'learning_rate': 1.3485046728971963e-05, 'epoch': 0.13}\n",
            "{'loss': 1.6152, 'learning_rate': 1.3480166147455868e-05, 'epoch': 0.13}\n",
            "{'loss': 1.5338, 'learning_rate': 1.3475285565939772e-05, 'epoch': 0.13}\n",
            "{'loss': 1.1857, 'learning_rate': 1.3470404984423677e-05, 'epoch': 0.13}\n",
            "{'loss': 1.5149, 'learning_rate': 1.346552440290758e-05, 'epoch': 0.13}\n",
            "{'loss': 1.5262, 'learning_rate': 1.3460643821391485e-05, 'epoch': 0.14}\n",
            "{'loss': 1.6426, 'learning_rate': 1.345576323987539e-05, 'epoch': 0.14}\n",
            "{'loss': 1.6226, 'learning_rate': 1.3450882658359295e-05, 'epoch': 0.14}\n",
            "{'loss': 1.3368, 'learning_rate': 1.3446002076843198e-05, 'epoch': 0.14}\n",
            "{'loss': 1.6333, 'learning_rate': 1.3441121495327103e-05, 'epoch': 0.14}\n",
            "{'loss': 1.7304, 'learning_rate': 1.3436240913811008e-05, 'epoch': 0.14}\n",
            "{'loss': 1.7113, 'learning_rate': 1.3431360332294912e-05, 'epoch': 0.14}\n",
            "{'loss': 1.7549, 'learning_rate': 1.3426479750778817e-05, 'epoch': 0.14}\n",
            "{'loss': 1.5653, 'learning_rate': 1.342159916926272e-05, 'epoch': 0.14}\n",
            "{'loss': 1.9134, 'learning_rate': 1.3416718587746625e-05, 'epoch': 0.15}\n",
            "{'loss': 1.5821, 'learning_rate': 1.341183800623053e-05, 'epoch': 0.15}\n",
            "{'loss': 1.5317, 'learning_rate': 1.3406957424714435e-05, 'epoch': 0.15}\n",
            "{'loss': 1.884, 'learning_rate': 1.3402076843198339e-05, 'epoch': 0.15}\n",
            "{'loss': 1.548, 'learning_rate': 1.3397196261682244e-05, 'epoch': 0.15}\n",
            "{'loss': 1.7194, 'learning_rate': 1.3392315680166149e-05, 'epoch': 0.15}\n",
            "{'loss': 1.3855, 'learning_rate': 1.3387435098650052e-05, 'epoch': 0.15}\n",
            "{'loss': 1.8105, 'learning_rate': 1.3382554517133957e-05, 'epoch': 0.15}\n",
            "{'loss': 1.4571, 'learning_rate': 1.337767393561786e-05, 'epoch': 0.15}\n",
            "{'loss': 1.9043, 'learning_rate': 1.3372793354101765e-05, 'epoch': 0.15}\n",
            "{'loss': 1.651, 'learning_rate': 1.336791277258567e-05, 'epoch': 0.16}\n",
            "{'loss': 1.5643, 'learning_rate': 1.3363032191069575e-05, 'epoch': 0.16}\n",
            "{'loss': 1.8425, 'learning_rate': 1.3358151609553479e-05, 'epoch': 0.16}\n",
            "{'loss': 2.2564, 'learning_rate': 1.3353271028037384e-05, 'epoch': 0.16}\n",
            "{'loss': 1.5281, 'learning_rate': 1.3348390446521289e-05, 'epoch': 0.16}\n",
            "{'loss': 1.2466, 'learning_rate': 1.3343509865005192e-05, 'epoch': 0.16}\n",
            "{'loss': 1.6111, 'learning_rate': 1.3338629283489097e-05, 'epoch': 0.16}\n",
            "{'loss': 2.0929, 'learning_rate': 1.3333748701973e-05, 'epoch': 0.16}\n",
            "{'loss': 1.536, 'learning_rate': 1.3328868120456906e-05, 'epoch': 0.16}\n",
            "{'loss': 1.6479, 'learning_rate': 1.332398753894081e-05, 'epoch': 0.17}\n",
            "{'loss': 2.0139, 'learning_rate': 1.3319106957424716e-05, 'epoch': 0.17}\n",
            "{'loss': 1.6796, 'learning_rate': 1.331422637590862e-05, 'epoch': 0.17}\n",
            "{'loss': 1.4855, 'learning_rate': 1.3309345794392524e-05, 'epoch': 0.17}\n",
            "{'loss': 1.8631, 'learning_rate': 1.3304465212876429e-05, 'epoch': 0.17}\n",
            "{'loss': 2.1647, 'learning_rate': 1.3299584631360332e-05, 'epoch': 0.17}\n",
            "{'loss': 1.8517, 'learning_rate': 1.3294704049844237e-05, 'epoch': 0.17}\n",
            "{'loss': 1.844, 'learning_rate': 1.328982346832814e-05, 'epoch': 0.17}\n",
            "{'loss': 1.2105, 'learning_rate': 1.3284942886812046e-05, 'epoch': 0.17}\n",
            "{'loss': 1.8408, 'learning_rate': 1.3280062305295951e-05, 'epoch': 0.17}\n",
            "{'loss': 1.1871, 'learning_rate': 1.3275181723779856e-05, 'epoch': 0.18}\n",
            "{'loss': 1.7639, 'learning_rate': 1.3270301142263761e-05, 'epoch': 0.18}\n",
            "{'loss': 1.7871, 'learning_rate': 1.3265420560747664e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8858, 'learning_rate': 1.326053997923157e-05, 'epoch': 0.18}\n",
            "{'loss': 1.7418, 'learning_rate': 1.3255659397715473e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8177, 'learning_rate': 1.3250778816199378e-05, 'epoch': 0.18}\n",
            "{'loss': 1.2585, 'learning_rate': 1.3245898234683281e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8067, 'learning_rate': 1.3241017653167186e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8359, 'learning_rate': 1.3236137071651091e-05, 'epoch': 0.18}\n",
            "{'loss': 1.4211, 'learning_rate': 1.3231256490134996e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8414, 'learning_rate': 1.3226375908618901e-05, 'epoch': 0.19}\n",
            "{'loss': 1.7707, 'learning_rate': 1.3221495327102804e-05, 'epoch': 0.19}\n",
            "{'loss': 1.4017, 'learning_rate': 1.321661474558671e-05, 'epoch': 0.19}\n",
            "{'loss': 1.7306, 'learning_rate': 1.3211734164070613e-05, 'epoch': 0.19}\n",
            "{'loss': 1.7711, 'learning_rate': 1.3206853582554518e-05, 'epoch': 0.19}\n",
            "{'loss': 1.7246, 'learning_rate': 1.3201973001038421e-05, 'epoch': 0.19}\n",
            "{'loss': 1.4905, 'learning_rate': 1.3197092419522326e-05, 'epoch': 0.19}\n",
            "{'loss': 1.8187, 'learning_rate': 1.3192211838006231e-05, 'epoch': 0.19}\n",
            "{'loss': 1.8012, 'learning_rate': 1.3187331256490136e-05, 'epoch': 0.19}\n",
            "{'loss': 1.7769, 'learning_rate': 1.3182450674974041e-05, 'epoch': 0.2}\n",
            "{'loss': 2.0127, 'learning_rate': 1.3177570093457945e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7149, 'learning_rate': 1.317268951194185e-05, 'epoch': 0.2}\n",
            "{'loss': 1.516, 'learning_rate': 1.3167808930425753e-05, 'epoch': 0.2}\n",
            "{'loss': 1.5932, 'learning_rate': 1.3162928348909658e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7265, 'learning_rate': 1.3158047767393561e-05, 'epoch': 0.2}\n",
            "{'loss': 1.8327, 'learning_rate': 1.3153167185877466e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7001, 'learning_rate': 1.314828660436137e-05, 'epoch': 0.2}\n",
            "{'loss': 1.6026, 'learning_rate': 1.3143406022845276e-05, 'epoch': 0.2}\n",
            "{'loss': 1.6475, 'learning_rate': 1.3138525441329181e-05, 'epoch': 0.2}\n",
            "  7% 197/2889 [23:03<4:42:15,  6.29s/it]Traceback (most recent call last):\n",
            "  File \"/content/llm-qa-rlhf/src/trl/sft_trainer.py\", line 123, in <module>\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2654, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2679, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 922, in forward\n",
            "    return self.base_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 806, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 685, in forward\n",
            "    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 681, in custom_forward\n",
            "    return module(*inputs, output_attentions, None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 421, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 216, in forward\n",
            "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 441, in forward\n",
            "    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\", line 563, in matmul\n",
            "    return MatMul8bitLt.apply(A, B, out, bias, state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\", line 327, in forward\n",
            "    CA, CAt, SCA, SCAt, coo_tensorA = F.double_quant(A.to(torch.float16), threshold=state.threshold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\", line 2016, in double_quant\n",
            "    nnz = nnz_row_ptr[-1].item()\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ec75f8b4f5da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"python llm-qa-rlhf/src/trl/sft_trainer.py      --model_name meta-llama/Llama-2-7b-hf      --dataset_name dave-does-data/databricks-dolly-qa-subset-7k      --log_with 'wandb'      --load_in_8bit      --use_peft      --batch_size 4      --gradient_accumulation_steps 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}